version: '2.4'
# version 3.x does not allow us to specify nvidia runtime for the analyzer node

services:
    analyzer:
      image: docker.io/tensorflow/serving:2.1.4-gpu
      runtime: nvidia
      ports:
        - 8500:8500
        - 8501:8501
      volumes:
        - type: bind
          source: <<path to the model model>>
          target: /models/mobilenet_v2
      environment:
        - MODEL_NAME=mobilenet_v2
        - CUDA_VISIBLE_DEVICES=0
      command: --enable_batching
    control:
        build: ./ControlNode
        image: control-node
        ports:
            - 8081:8081
        volumes:
            - type: bind
              source: <<local path to list of video paths>>
              target: /usr/config/Paths.txt
            - type: bind
              source: <<local directory to save log files in>>
              target: /usr/logs
        command: --paths /usr/config/Paths.txt --logDir /usr/logs
    processor:
        build: .
        image: snva-processor
        depends_on:
          - "analyzer"
          - "control"
        volumes:
            - type: bind
              source: <<parent directory of model (should contain class_names.txt)>>
              target: /usr/model
            - type: bind
              source: <<path to output directory>>
              target: /usr/output
            - type: bind
              source: <<local directory containing video files>>
              target: /usr/videos
            - type: bind
              source: /home/bsumner/Documents/testLogs
              target: /usr/logs
        command: -et -cnh 0.0.0.0:8081 -msh 0.0.0.0:8500 -wir true

